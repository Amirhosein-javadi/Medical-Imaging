{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "29xJBEtm7LSG",
        "qQRjcHGf7fMv",
        "FI847wXY7qAt",
        "_d02eE8H764N",
        "3AFCkFud7_Rk",
        "7QHWf49R8HeG",
        "Sipfnw_Y8Q4G"
      ],
      "mount_file_id": "1XMJtWsJ0wOgog1JEHVrROQyrENLv9QgY",
      "authorship_tag": "ABX9TyOLiXP42sY6g3vQub3KlErd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhosein-javadi/Medical-Imaging/blob/main/UNet_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "rK5Fil0Sdzu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/MedicalImage-Team/LiTS17/Info.txt') as f:\n",
        "  lines = f.readlines()"
      ],
      "metadata": {
        "id": "CpI866K8d2e-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_properties = []\n",
        "\n",
        "for i in range(2,len(lines)):\n",
        "  line = lines[i]\n",
        "  line_split = line.split()\n",
        "  dana_num = int(line_split[0])\n",
        "  first_layer = int(line_split[5][1:-1]) \n",
        "  last_later = int(line_split[-1][0:-1])\n",
        "  data_dict = {\"dana_num\" : dana_num,\n",
        "               \"first_layer\" : first_layer,\n",
        "               \"last_layer\" : last_later\n",
        "               }\n",
        "  layer_properties.append(data_dict)\n",
        "\n",
        "layer_properties;"
      ],
      "metadata": {
        "id": "1nkqEV5SH-fu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "Sipfnw_Y8Q4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "\n",
        "plt.ioff()\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_dir, logger=None):\n",
        "    \"\"\"Saves model and training parameters at '{checkpoint_dir}/last_checkpoint.pytorch'.\n",
        "    If is_best==True saves '{checkpoint_dir}/best_checkpoint.pytorch' as well.\n",
        "\n",
        "    Args:\n",
        "        state (dict): contains model's state_dict, optimizer's state_dict, epoch\n",
        "            and best evaluation metric value so far\n",
        "        is_best (bool): if True state contains the best model seen so far\n",
        "        checkpoint_dir (string): directory where the checkpoint are to be saved\n",
        "    \"\"\"\n",
        "\n",
        "    def log_info(message):\n",
        "        if logger is not None:\n",
        "            logger.info(message)\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        log_info(\n",
        "            f\"Checkpoint directory does not exists. Creating {checkpoint_dir}\")\n",
        "        os.mkdir(checkpoint_dir)\n",
        "\n",
        "    last_file_path = os.path.join(checkpoint_dir, 'last_checkpoint.pytorch')\n",
        "    log_info(f\"Saving last checkpoint to '{last_file_path}'\")\n",
        "    torch.save(state, last_file_path)\n",
        "    if is_best:\n",
        "        best_file_path = os.path.join(checkpoint_dir, 'best_checkpoint.pytorch')\n",
        "        log_info(f\"Saving best checkpoint to '{best_file_path}'\")\n",
        "        shutil.copyfile(last_file_path, best_file_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer=None,\n",
        "                    model_key='model_state_dict', optimizer_key='optimizer_state_dict'):\n",
        "    \"\"\"Loads model and training parameters from a given checkpoint_path\n",
        "    If optimizer is provided, loads optimizer's state_dict of as well.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (string): path to the checkpoint to be loaded\n",
        "        model (torch.nn.Module): model into which the parameters are to be copied\n",
        "        optimizer (torch.optim.Optimizer) optional: optimizer instance into\n",
        "            which the parameters are to be copied\n",
        "\n",
        "    Returns:\n",
        "        state\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise IOError(f\"Checkpoint '{checkpoint_path}' does not exist\")\n",
        "\n",
        "    state = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(state[model_key])\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(state[optimizer_key])\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def save_network_output(output_path, output, logger=None):\n",
        "    if logger is not None:\n",
        "        logger.info(f'Saving network output to: {output_path}...')\n",
        "    output = output.detach().cpu()[0]\n",
        "    with h5py.File(output_path, 'w') as f:\n",
        "        f.create_dataset('predictions', data=output, compression='gzip')\n",
        "\n",
        "\n",
        "loggers = {}\n",
        "\n",
        "\n",
        "def get_logger(name, level=logging.INFO):\n",
        "    global loggers\n",
        "    if loggers.get(name) is not None:\n",
        "        return loggers[name]\n",
        "    else:\n",
        "        logger = logging.getLogger(name)\n",
        "        logger.setLevel(level)\n",
        "        # Logging to console\n",
        "        stream_handler = logging.StreamHandler(sys.stdout)\n",
        "        formatter = logging.Formatter(\n",
        "            '%(asctime)s [%(threadName)s] %(levelname)s %(name)s - %(message)s')\n",
        "        stream_handler.setFormatter(formatter)\n",
        "        logger.addHandler(stream_handler)\n",
        "\n",
        "        loggers[name] = logger\n",
        "\n",
        "        return logger\n",
        "\n",
        "\n",
        "def get_number_of_learnable_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    return sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "\n",
        "class RunningAverage:\n",
        "    \"\"\"Computes and stores the average\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.sum = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.count += n\n",
        "        self.sum += value * n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def find_maximum_patch_size(model, device):\n",
        "    \"\"\"Tries to find the biggest patch size that can be send to GPU for inference\n",
        "    without throwing CUDA out of memory\"\"\"\n",
        "    logger = get_logger('PatchFinder')\n",
        "    in_channels = model.in_channels\n",
        "\n",
        "    patch_shapes = [(64, 128, 128), (96, 128, 128),\n",
        "                    (64, 160, 160), (96, 160, 160),\n",
        "                    (64, 192, 192), (96, 192, 192)]\n",
        "\n",
        "    for shape in patch_shapes:\n",
        "        # generate random patch of a given size\n",
        "        patch = np.random.randn(*shape).astype('float32')\n",
        "\n",
        "        patch = torch \\\n",
        "            .from_numpy(patch) \\\n",
        "            .view((1, in_channels) + patch.shape) \\\n",
        "            .to(device)\n",
        "\n",
        "        logger.info(f\"Current patch size: {shape}\")\n",
        "        model(patch)\n",
        "\n",
        "\n",
        "def remove_halo(patch, index, shape, patch_halo):\n",
        "    \"\"\"\n",
        "    Remove `pad_width` voxels around the edges of a given patch.\n",
        "    \"\"\"\n",
        "    assert len(patch_halo) == 3\n",
        "\n",
        "    def _new_slices(slicing, max_size, pad):\n",
        "        if slicing.start == 0:\n",
        "            p_start = 0\n",
        "            i_start = 0\n",
        "        else:\n",
        "            p_start = pad\n",
        "            i_start = slicing.start + pad\n",
        "\n",
        "        if slicing.stop == max_size:\n",
        "            p_stop = None\n",
        "            i_stop = max_size\n",
        "        else:\n",
        "            p_stop = -pad if pad != 0 else 1\n",
        "            i_stop = slicing.stop - pad\n",
        "\n",
        "        return slice(p_start, p_stop), slice(i_start, i_stop)\n",
        "\n",
        "    D, H, W = shape\n",
        "\n",
        "    i_c, i_z, i_y, i_x = index\n",
        "    p_c = slice(0, patch.shape[0])\n",
        "\n",
        "    p_z, i_z = _new_slices(i_z, D, patch_halo[0])\n",
        "    p_y, i_y = _new_slices(i_y, H, patch_halo[1])\n",
        "    p_x, i_x = _new_slices(i_x, W, patch_halo[2])\n",
        "\n",
        "    patch_index = (p_c, p_z, p_y, p_x)\n",
        "    index = (i_c, i_z, i_y, i_x)\n",
        "    return patch[patch_index], index\n",
        "\n",
        "\n",
        "def number_of_features_per_level(init_channel_number, num_levels):\n",
        "    return [init_channel_number * 2 ** k for k in range(num_levels)]\n",
        "\n",
        "\n",
        "class _TensorboardFormatter:\n",
        "    \"\"\"\n",
        "    Tensorboard formatters converts a given batch of images (be it input/output to the network or the target segmentation\n",
        "    image) to a series of images that can be displayed in tensorboard. This is the parent class for all tensorboard\n",
        "    formatters which ensures that returned images are in the 'CHW' format.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, name, batch):\n",
        "        \"\"\"\n",
        "        Transform a batch to a series of tuples of the form (tag, img), where `tag` corresponds to the image tag\n",
        "        and `img` is the image itself.\n",
        "\n",
        "        Args:\n",
        "             name (str): one of 'inputs'/'targets'/'predictions'\n",
        "             batch (torch.tensor): 4D or 5D torch tensor\n",
        "        \"\"\"\n",
        "\n",
        "        def _check_img(tag_img):\n",
        "            tag, img = tag_img\n",
        "\n",
        "            assert img.ndim == 2 or img.ndim == 3, 'Only 2D (HW) and 3D (CHW) images are accepted for display'\n",
        "\n",
        "            if img.ndim == 2:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            else:\n",
        "                C = img.shape[0]\n",
        "                assert C == 1 or C == 3, 'Only (1, H, W) or (3, H, W) images are supported'\n",
        "\n",
        "            return tag, img\n",
        "\n",
        "        tagged_images = self.process_batch(name, batch)\n",
        "\n",
        "        return list(map(_check_img, tagged_images))\n",
        "\n",
        "    def process_batch(self, name, batch):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class DefaultTensorboardFormatter(_TensorboardFormatter):\n",
        "    def __init__(self, skip_last_target=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.skip_last_target = skip_last_target\n",
        "\n",
        "    def process_batch(self, name, batch):\n",
        "        if name == 'targets' and self.skip_last_target:\n",
        "            batch = batch[:, :-1, ...]\n",
        "\n",
        "        tag_template = '{}/batch_{}/channel_{}/slice_{}'\n",
        "\n",
        "        tagged_images = []\n",
        "\n",
        "        if batch.ndim == 5:\n",
        "            # NCDHW\n",
        "            slice_idx = batch.shape[2] // 2  # get the middle slice\n",
        "            for batch_idx in range(batch.shape[0]):\n",
        "                for channel_idx in range(batch.shape[1]):\n",
        "                    tag = tag_template.format(name, batch_idx, channel_idx, slice_idx)\n",
        "                    img = batch[batch_idx, channel_idx, slice_idx, ...]\n",
        "                    tagged_images.append((tag, self._normalize_img(img)))\n",
        "        else:\n",
        "            # batch has no channel dim: NDHW\n",
        "            slice_idx = batch.shape[1] // 2  # get the middle slice\n",
        "            for batch_idx in range(batch.shape[0]):\n",
        "                tag = tag_template.format(name, batch_idx, 0, slice_idx)\n",
        "                img = batch[batch_idx, slice_idx, ...]\n",
        "                tagged_images.append((tag, self._normalize_img(img)))\n",
        "\n",
        "        return tagged_images\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_img(img):\n",
        "        return np.nan_to_num((img - np.min(img)) / np.ptp(img))\n",
        "\n",
        "\n",
        "def _find_masks(batch, min_size=10):\n",
        "    \"\"\"Center the z-slice in the 'middle' of a given instance, given a batch of instances\n",
        "\n",
        "    Args:\n",
        "        batch (ndarray): 5d numpy tensor (NCDHW)\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for b in batch:\n",
        "        assert b.shape[0] == 1\n",
        "        patch = b[0]\n",
        "        z_sum = patch.sum(axis=(1, 2))\n",
        "        coords = np.where(z_sum > min_size)[0]\n",
        "        if len(coords) > 0:\n",
        "            ind = coords[len(coords) // 2]\n",
        "            result.append(b[:, ind:ind + 1, ...])\n",
        "        else:\n",
        "            ind = b.shape[1] // 2\n",
        "            result.append(b[:, ind:ind + 1, ...])\n",
        "\n",
        "    return np.stack(result, axis=0)\n",
        "\n",
        "\n",
        "def get_tensorboard_formatter(config):\n",
        "    if config is None:\n",
        "        return DefaultTensorboardFormatter()\n",
        "\n",
        "    class_name = config['name']\n",
        "    m = importlib.import_module('pytorch3dunet.unet3d.utils')\n",
        "    clazz = getattr(m, class_name)\n",
        "    return clazz(**config)\n",
        "\n",
        "\n",
        "def expand_as_one_hot(input, C, ignore_index=None):\n",
        "    \"\"\"\n",
        "    Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\n",
        "    It is assumed that the batch dimension is present.\n",
        "    Args:\n",
        "        input (torch.Tensor): 3D/4D input image\n",
        "        C (int): number of channels/labels\n",
        "        ignore_index (int): ignore index to be kept during the expansion\n",
        "    Returns:\n",
        "        4D/5D output torch.Tensor (NxCxSPATIAL)\n",
        "    \"\"\"\n",
        "    assert input.dim() == 4\n",
        "\n",
        "    # expand the input tensor to Nx1xSPATIAL before scattering\n",
        "    input = input.unsqueeze(1)\n",
        "    # create output tensor shape (NxCxSPATIAL)\n",
        "    shape = list(input.size())\n",
        "    shape[1] = C\n",
        "\n",
        "    if ignore_index is not None:\n",
        "        # create ignore_index mask for the result\n",
        "        mask = input.expand(shape) == ignore_index\n",
        "        # clone the src tensor and zero out ignore_index in the input\n",
        "        input = input.clone()\n",
        "        input[input == ignore_index] = 0\n",
        "        # scatter to get the one-hot tensor\n",
        "        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
        "        # bring back the ignore_index in the result\n",
        "        result[mask] = ignore_index\n",
        "        return result\n",
        "    else:\n",
        "        # scatter to get the one-hot tensor\n",
        "        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
        "\n",
        "\n",
        "def convert_to_numpy(*inputs):\n",
        "    \"\"\"\n",
        "    Coverts input tensors to numpy ndarrays\n",
        "\n",
        "    Args:\n",
        "        inputs (iteable of torch.Tensor): torch tensor\n",
        "\n",
        "    Returns:\n",
        "        tuple of ndarrays\n",
        "    \"\"\"\n",
        "\n",
        "    def _to_numpy(i):\n",
        "        assert isinstance(i, torch.Tensor), \"Expected input to be torch.Tensor\"\n",
        "        return i.detach().cpu().numpy()\n",
        "\n",
        "    return (_to_numpy(i) for i in inputs)\n",
        "\n",
        "\n",
        "def create_optimizer(optimizer_config, model):\n",
        "    learning_rate = optimizer_config['learning_rate']\n",
        "    weight_decay = optimizer_config.get('weight_decay', 0)\n",
        "    betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def create_lr_scheduler(lr_config, optimizer):\n",
        "    if lr_config is None:\n",
        "        return None\n",
        "    class_name = lr_config.pop('name')\n",
        "    m = importlib.import_module('torch.optim.lr_scheduler')\n",
        "    clazz = getattr(m, class_name)\n",
        "    # add optimizer to the config\n",
        "    lr_config['optimizer'] = optimizer\n",
        "    return clazz(**lr_config)\n",
        "\n",
        "\n",
        "def create_sample_plotter(sample_plotter_config):\n",
        "    if sample_plotter_config is None:\n",
        "        return None\n",
        "    class_name = sample_plotter_config['name']\n",
        "    m = importlib.import_module('pytorch3dunet.unet3d.utils')\n",
        "    clazz = getattr(m, class_name)\n",
        "    return clazz(**sample_plotter_config)\n"
      ],
      "metadata": {
        "id": "U7IGOzaY8ZWM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## buildingblocks"
      ],
      "metadata": {
        "id": "29xJBEtm7LSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def conv3d(in_channels, out_channels, kernel_size, bias, padding):\n",
        "    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
        "\n",
        "\n",
        "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n",
        "    \"\"\"\n",
        "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
        "    and optional batchnorm/groupnorm.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        kernel_size(int or tuple): size of the convolving kernel\n",
        "        order (string): order of things, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'gcr' -> groupnorm + conv + ReLU\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "            'bcr' -> batchnorm + conv + ReLU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "\n",
        "    Return:\n",
        "        list of tuple (name, module)\n",
        "    \"\"\"\n",
        "    assert 'c' in order, \"Conv layer MUST be present\"\n",
        "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
        "\n",
        "    modules = []\n",
        "    for i, char in enumerate(order):\n",
        "        if char == 'r':\n",
        "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
        "        elif char == 'l':\n",
        "            modules.append(('LeakyReLU', nn.LeakyReLU(inplace=True)))\n",
        "        elif char == 'e':\n",
        "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
        "        elif char == 'c':\n",
        "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
        "            bias = not ('g' in order or 'b' in order)\n",
        "            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
        "        elif char == 'g':\n",
        "            is_before_conv = i < order.index('c')\n",
        "            if is_before_conv:\n",
        "                num_channels = in_channels\n",
        "            else:\n",
        "                num_channels = out_channels\n",
        "\n",
        "            # use only one group if the given number of groups is greater than the number of channels\n",
        "            if num_channels < num_groups:\n",
        "                num_groups = 1\n",
        "\n",
        "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
        "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
        "        elif char == 'b':\n",
        "            is_before_conv = i < order.index('c')\n",
        "            if is_before_conv:\n",
        "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
        "            else:\n",
        "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
        "\n",
        "    return modules\n",
        "\n",
        "\n",
        "class SingleConv(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
        "    of operations can be specified via the `order` parameter\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "        order (string): determines the order of layers, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'crg' -> conv + ReLU + groupnorm\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple):\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
        "        super(SingleConv, self).__init__()\n",
        "\n",
        "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
        "            self.add_module(name, module)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Sequential):\n",
        "    \"\"\"\n",
        "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
        "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
        "    This can be changed however by providing the 'order' argument, e.g. in order\n",
        "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
        "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
        "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "        order (string): determines the order of layers, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'crg' -> conv + ReLU + groupnorm\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        if encoder:\n",
        "            # we're in the encoder path\n",
        "            conv1_in_channels = in_channels\n",
        "            conv1_out_channels = out_channels // 2\n",
        "            if conv1_out_channels < in_channels:\n",
        "                conv1_out_channels = in_channels\n",
        "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
        "        else:\n",
        "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
        "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
        "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
        "\n",
        "        # conv1\n",
        "        self.add_module('SingleConv1',\n",
        "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
        "                                   padding=padding))\n",
        "        # conv2\n",
        "        self.add_module('SingleConv2',\n",
        "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
        "                                   padding=padding))\n",
        "\n",
        "\n",
        "class ExtResNetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic UNet block consisting of a SingleConv followed by the residual block.\n",
        "    The SingleConv takes care of increasing/decreasing the number of channels and also ensures that the number\n",
        "    of output channels is compatible with the residual block that follows.\n",
        "    This block can be used instead of standard DoubleConv in the Encoder module.\n",
        "    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n",
        "\n",
        "    Notice we use ELU instead of ReLU (order='cge') and put non-linearity after the groupnorm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, **kwargs):\n",
        "        super(ExtResNetBlock, self).__init__()\n",
        "\n",
        "        # first convolution\n",
        "        self.conv1 = SingleConv(in_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
        "        # residual block\n",
        "        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
        "        # remove non-linearity from the 3rd convolution since it's going to be applied after adding the residual\n",
        "        n_order = order\n",
        "        for c in 'rel':\n",
        "            n_order = n_order.replace(c, '')\n",
        "        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n",
        "                                num_groups=num_groups)\n",
        "\n",
        "        # create non-linearity separately\n",
        "        if 'l' in order:\n",
        "            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        elif 'e' in order:\n",
        "            self.non_linearity = nn.ELU(inplace=True)\n",
        "        else:\n",
        "            self.non_linearity = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply first convolution and save the output as a residual\n",
        "        out = self.conv1(x)\n",
        "        residual = out\n",
        "        #print(\"out1\" , out.shape)\n",
        "\n",
        "        # residual block\n",
        "        out = self.conv2(out)\n",
        "        #print(\"out2\" , out.shape)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        #print(\"out3\" , out.shape)\n",
        "\n",
        "        out += residual\n",
        "        out = self.non_linearity(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A single module from the encoder path consisting of the optional max\n",
        "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
        "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
        "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
        "    a DoubleConv module.\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
        "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
        "        pool_kernel_size (int or tuple): the size of the window\n",
        "        pool_type (str): pooling layer: 'max' or 'avg'\n",
        "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
        "        conv_layer_order (string): determines the order of layers\n",
        "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
        "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='gcr',\n",
        "                 num_groups=8, padding=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        assert pool_type in ['max', 'avg']\n",
        "        if apply_pooling:\n",
        "            if pool_type == 'max':\n",
        "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
        "            else:\n",
        "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
        "        else:\n",
        "            self.pooling = None\n",
        "\n",
        "        self.basic_module = basic_module(in_channels, out_channels,\n",
        "                                         encoder=True,\n",
        "                                         kernel_size=conv_kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups,\n",
        "                                         padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pooling is not None:\n",
        "            x = self.pooling(x)\n",
        "        x = self.basic_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A single module for decoder path consisting of the upsampling layer\n",
        "    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
        "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
        "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
        "            from the corresponding encoder\n",
        "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
        "        conv_layer_order (string): determines the order of layers\n",
        "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "        upsample (boole): should the input be upsampled\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n",
        "                 conv_layer_order='gcr', num_groups=8, mode='nearest', padding=1, upsample=True):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        if upsample:\n",
        "            if basic_module == DoubleConv:\n",
        "                # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n",
        "                self.upsampling = InterpolateUpsampling(mode=mode)\n",
        "                # concat joining\n",
        "                self.joining = partial(self._joining, concat=True)\n",
        "            else:\n",
        "                # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
        "                self.upsampling = TransposeConvUpsampling(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                          kernel_size=conv_kernel_size, scale_factor=scale_factor)\n",
        "                # sum joining\n",
        "                self.joining = partial(self._joining, concat=False)\n",
        "                # adapt the number of in_channels for the ExtResNetBlock\n",
        "                in_channels = out_channels\n",
        "        else:\n",
        "            # no upsampling\n",
        "            self.upsampling = NoUpsampling()\n",
        "            # concat joining\n",
        "            self.joining = partial(self._joining, concat=True)\n",
        "\n",
        "        self.basic_module = basic_module(in_channels, out_channels,\n",
        "                                         encoder=False,\n",
        "                                         kernel_size=conv_kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups,\n",
        "                                         padding=padding)\n",
        "\n",
        "    def forward(self, encoder_features, x):\n",
        "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
        "        #print(\"upsampling:\" , x.shape)\n",
        "        x = self.joining(encoder_features, x)\n",
        "        #print(\"joining:\" , x.shape)\n",
        "        x = self.basic_module(x)\n",
        "        #print(\"basic_module\" , x.shape)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _joining(encoder_features, x, concat):\n",
        "        if concat:\n",
        "            return torch.cat((encoder_features, x), dim=1)\n",
        "        else:\n",
        "            return encoder_features + x\n",
        "\n",
        "\n",
        "def create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
        "                    pool_kernel_size):\n",
        "    # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
        "    encoders = []\n",
        "    for i, out_feature_num in enumerate(f_maps):\n",
        "        if i == 0:\n",
        "            encoder = Encoder(in_channels, out_feature_num,\n",
        "                              apply_pooling=False,  # skip pooling in the firs encoder\n",
        "                              basic_module=basic_module,\n",
        "                              conv_layer_order=layer_order,\n",
        "                              conv_kernel_size=conv_kernel_size,\n",
        "                              num_groups=num_groups,\n",
        "                              padding=conv_padding)\n",
        "            \n",
        "        else:\n",
        "            # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n",
        "            encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
        "                              basic_module=basic_module,\n",
        "                              conv_layer_order=layer_order,\n",
        "                              conv_kernel_size=conv_kernel_size,\n",
        "                              num_groups=num_groups,\n",
        "                              pool_kernel_size=pool_kernel_size,\n",
        "                              padding=conv_padding)\n",
        "\n",
        "        encoders.append(encoder)\n",
        "\n",
        "    return nn.ModuleList(encoders)\n",
        "\n",
        "\n",
        "def create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups, upsample):\n",
        "    # create decoder path consisting of the Decoder modules. The length of the decoder list is equal to `len(f_maps) - 1`\n",
        "    decoders = []\n",
        "    reversed_f_maps = list(reversed(f_maps))\n",
        "    for i in range(len(reversed_f_maps) - 1):\n",
        "        if basic_module == DoubleConv:\n",
        "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
        "        else:\n",
        "            in_feature_num = reversed_f_maps[i]\n",
        "\n",
        "        out_feature_num = reversed_f_maps[i + 1]\n",
        "\n",
        "        # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n",
        "        # currently strides with a constant stride: (2, 2, 2)\n",
        "\n",
        "        _upsample = True\n",
        "        if i == 0:\n",
        "            # upsampling can be skipped only for the 1st decoder, afterwards it should always be present\n",
        "            _upsample = upsample\n",
        "\n",
        "        decoder = Decoder(in_feature_num, out_feature_num,\n",
        "                          basic_module=basic_module,\n",
        "                          conv_layer_order=layer_order,\n",
        "                          conv_kernel_size=conv_kernel_size,\n",
        "                          num_groups=num_groups,\n",
        "                          padding=conv_padding,\n",
        "                          upsample=_upsample)\n",
        "        decoders.append(decoder)\n",
        "    return nn.ModuleList(decoders)\n",
        "\n",
        "\n",
        "class AbstractUpsampling(nn.Module):\n",
        "    \"\"\"\n",
        "    Abstract class for upsampling. A given implementation should upsample a given 5D input tensor using either\n",
        "    interpolation or learned transposed convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, upsample):\n",
        "        super(AbstractUpsampling, self).__init__()\n",
        "        self.upsample = upsample\n",
        "\n",
        "    def forward(self, encoder_features, x):\n",
        "        # get the spatial dimensions of the output given the encoder_features\n",
        "        output_size = encoder_features.size()[2:]\n",
        "        # upsample the input and return\n",
        "        return self.upsample(x, output_size)\n",
        "\n",
        "\n",
        "class InterpolateUpsampling(AbstractUpsampling):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        mode (str): algorithm used for upsampling:\n",
        "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
        "            used only if transposed_conv is False\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='nearest'):\n",
        "        upsample = partial(self._interpolate, mode=mode)\n",
        "        super().__init__(upsample)\n",
        "\n",
        "    @staticmethod\n",
        "    def _interpolate(x, size, mode):\n",
        "        return F.interpolate(x, size=size, mode=mode)\n",
        "\n",
        "\n",
        "class TransposeConvUpsampling(AbstractUpsampling):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        in_channels (int): number of input channels for transposed conv\n",
        "            used only if transposed_conv is True\n",
        "        out_channels (int): number of output channels for transpose conv\n",
        "            used only if transposed_conv is True\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "            used only if transposed_conv is True\n",
        "        scale_factor (int or tuple): stride of the convolution\n",
        "            used only if transposed_conv is True\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=None, out_channels=None, kernel_size=3, scale_factor=(2, 2, 2)):\n",
        "        # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
        "        upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n",
        "                                      padding=1)\n",
        "        super().__init__(upsample)\n",
        "\n",
        "\n",
        "class NoUpsampling(AbstractUpsampling):\n",
        "    def __init__(self):\n",
        "        super().__init__(self._no_upsampling)\n",
        "\n",
        "    @staticmethod\n",
        "    def _no_upsampling(x, size):\n",
        "        return x"
      ],
      "metadata": {
        "id": "M57dVvM2PXQm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "qQRjcHGf7fMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import MSELoss, SmoothL1Loss, L1Loss\n",
        "\n",
        "# from utils import expand_as_one_hot\n",
        "\n",
        "\n",
        "def compute_per_channel_dice(input, target, epsilon=1e-6, weight=None):\n",
        "    \"\"\"\n",
        "    Computes DiceCoefficient as defined in https://arxiv.org/abs/1606.04797 given  a multi channel input and target.\n",
        "    Assumes the input is a normalized probability, e.g. a result of Sigmoid or Softmax function.\n",
        "\n",
        "    Args:\n",
        "         input (torch.Tensor): NxCxSpatial input tensor\n",
        "         target (torch.Tensor): NxCxSpatial target tensor\n",
        "         epsilon (float): prevents division by zero\n",
        "         weight (torch.Tensor): Cx1 tensor of weight per channel/class\n",
        "    \"\"\"\n",
        "\n",
        "    # input and target shapes must match\n",
        "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
        "\n",
        "    input = flatten(input)\n",
        "    target = flatten(target)\n",
        "    target = target.float()\n",
        "\n",
        "    # compute per channel Dice Coefficient\n",
        "    intersect = (input * target).sum(-1)\n",
        "    if weight is not None:\n",
        "        intersect = weight * intersect\n",
        "\n",
        "    # here we can use standard dice (input + target).sum(-1) or extension (see V-Net) (input^2 + target^2).sum(-1)\n",
        "    denominator = (input * input).sum(-1) + (target * target).sum(-1)\n",
        "\n",
        "    return (2 * (intersect / denominator.clamp(min=epsilon)))\n",
        "\n",
        "\n",
        "class _MaskingLossWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss wrapper which prevents the gradient of the loss to be computed where target is equal to `ignore_index`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss, ignore_index):\n",
        "        super(_MaskingLossWrapper, self).__init__()\n",
        "        assert ignore_index is not None, 'ignore_index cannot be None'\n",
        "        self.loss = loss\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        mask = target.clone().ne_(self.ignore_index)\n",
        "        mask.requires_grad = False\n",
        "\n",
        "        # mask out input/target so that the gradient is zero where on the mask\n",
        "        input = input * mask\n",
        "        target = target * mask\n",
        "\n",
        "        # forward masked input and target to the loss\n",
        "        return self.loss(input, target)\n",
        "\n",
        "\n",
        "class SkipLastTargetChannelWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss wrapper which removes additional target channel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss, squeeze_channel=False):\n",
        "        super(SkipLastTargetChannelWrapper, self).__init__()\n",
        "        self.loss = loss\n",
        "        self.squeeze_channel = squeeze_channel\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        assert target.size(1) > 1, 'Target tensor has a singleton channel dimension, cannot remove channel'\n",
        "\n",
        "        # skips last target channel if needed\n",
        "        target = target[:, :-1, ...]\n",
        "\n",
        "        if self.squeeze_channel:\n",
        "            # squeeze channel dimension if singleton\n",
        "            target = torch.squeeze(target, dim=1)\n",
        "        return self.loss(input, target)\n",
        "\n",
        "\n",
        "class _AbstractDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for different implementations of Dice loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, normalization='sigmoid'):\n",
        "        super(_AbstractDiceLoss, self).__init__()\n",
        "        self.register_buffer('weight', weight)\n",
        "        # The output from the network during training is assumed to be un-normalized probabilities and we would\n",
        "        # like to normalize the logits. Since Dice (or soft Dice in this case) is usually used for binary data,\n",
        "        # normalizing the channels with Sigmoid is the default choice even for multi-class segmentation problems.\n",
        "        # However if one would like to apply Softmax in order to get the proper probability distribution from the\n",
        "        # output, just specify `normalization=Softmax`\n",
        "        assert normalization in ['sigmoid', 'softmax', 'none']\n",
        "        if normalization == 'sigmoid':\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        elif normalization == 'softmax':\n",
        "            self.normalization = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            self.normalization = lambda x: x\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        # actual Dice score computation; to be implemented by the subclass\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # get probabilities from logits\n",
        "        input = self.normalization(input)\n",
        "\n",
        "        # compute per channel Dice coefficient\n",
        "        per_channel_dice = self.dice(input, target, weight=self.weight)\n",
        "\n",
        "        # average Dice score across all channels/classes\n",
        "        return 1. - torch.mean(per_channel_dice)\n",
        "\n",
        "\n",
        "class DiceLoss(_AbstractDiceLoss):\n",
        "    \"\"\"Computes Dice Loss according to https://arxiv.org/abs/1606.04797.\n",
        "    For multi-class segmentation `weight` parameter can be used to assign different weights per class.\n",
        "    The input to the loss function is assumed to be a logit and will be normalized by the Sigmoid function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, normalization='sigmoid'):\n",
        "        super().__init__(weight, normalization)\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        return compute_per_channel_dice(input, target, weight=self.weight)[0]\n",
        "\n",
        "\n",
        "class GeneralizedDiceLoss(_AbstractDiceLoss):\n",
        "    \"\"\"Computes Generalized Dice Loss (GDL) as described in https://arxiv.org/pdf/1707.03237.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalization='sigmoid', epsilon=1e-6):\n",
        "        super().__init__(weight=None, normalization=normalization)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
        "\n",
        "        input = flatten(input)\n",
        "        target = flatten(target)\n",
        "        target = target.float()\n",
        "\n",
        "        if input.size(0) == 1:\n",
        "            # for GDL to make sense we need at least 2 channels (see https://arxiv.org/pdf/1707.03237.pdf)\n",
        "            # put foreground and background voxels in separate channels\n",
        "            input = torch.cat((input, 1 - input), dim=0)\n",
        "            target = torch.cat((target, 1 - target), dim=0)\n",
        "\n",
        "        # GDL weighting: the contribution of each label is corrected by the inverse of its volume\n",
        "        w_l = target.sum(-1)\n",
        "        w_l = 1 / (w_l * w_l).clamp(min=self.epsilon)\n",
        "        w_l.requires_grad = False\n",
        "\n",
        "        intersect = (input * target).sum(-1)\n",
        "        intersect = intersect * w_l\n",
        "\n",
        "        denominator = (input + target).sum(-1)\n",
        "        denominator = (denominator * w_l).clamp(min=self.epsilon)\n",
        "\n",
        "        return 2 * (intersect.sum() / denominator.sum())\n",
        "\n",
        "\n",
        "def focal_loss(bce_loss, targets, gamma, alpha):\n",
        "    \"\"\"Binary focal loss, mean.\n",
        "\n",
        "    Per https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/5 with\n",
        "    improvements for alpha.\n",
        "    :param bce_loss: Binary Cross Entropy loss, a torch tensor.\n",
        "    :param targets: a torch tensor containing the ground truth, 0s and 1s.\n",
        "    :param gamma: focal loss power parameter, a float scalar.\n",
        "    :param alpha: weight of the class indicated by 1, a float scalar.\n",
        "    \"\"\"\n",
        "    p_t = torch.exp(-bce_loss)\n",
        "    alpha_tensor = (1 - alpha) + targets * (2 * alpha - 1)  # alpha if target = 1 and 1 - alpha if target = 0\n",
        "    f_loss = alpha_tensor * (1 - p_t) ** gamma * bce_loss\n",
        "    return f_loss.mean()\n",
        "\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    \"\"\"Linear combination of BCE and Dice losses\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, beta):\n",
        "        super(BCEDiceLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.beta = beta\n",
        "        self.dice = DiceLoss()\n",
        "        self.mse = MSELoss()\n",
        "        \n",
        "\n",
        "    def forward(self, input, target):\n",
        "\n",
        "        return self.alpha * self.bce(input, target) + self.beta * self.dice(input, target) + focal_loss(self.bce(input, target) , target , gamma = 0.5 , alpha = 0.3)\n",
        "\n",
        "\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    \"\"\"WeightedCrossEntropyLoss (WCE) as described in https://arxiv.org/pdf/1707.03237.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-1):\n",
        "        super(WeightedCrossEntropyLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        weight = self._class_weights(input)\n",
        "        return F.cross_entropy(input, target, weight=weight, ignore_index=self.ignore_index)\n",
        "\n",
        "    @staticmethod\n",
        "    def _class_weights(input):\n",
        "        # normalize the input first\n",
        "        input = F.softmax(input, dim=1)\n",
        "        flattened = flatten(input)\n",
        "        nominator = (1. - flattened).sum(-1)\n",
        "        denominator = flattened.sum(-1)\n",
        "        class_weights = Variable(nominator / denominator, requires_grad=False)\n",
        "        return class_weights\n",
        "\n",
        "\n",
        "class PixelWiseCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, class_weights=None, ignore_index=None):\n",
        "        super(PixelWiseCrossEntropyLoss, self).__init__()\n",
        "        self.register_buffer('class_weights', class_weights)\n",
        "        self.ignore_index = ignore_index\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, target, weights):\n",
        "        assert target.size() == weights.size()\n",
        "        # normalize the input\n",
        "        log_probabilities = self.log_softmax(input)\n",
        "        # standard CrossEntropyLoss requires the target to be (NxDxHxW), so we need to expand it to (NxCxDxHxW)\n",
        "        target = expand_as_one_hot(target, C=input.size()[1], ignore_index=self.ignore_index)\n",
        "        # expand weights\n",
        "        weights = weights.unsqueeze(1)\n",
        "        weights = weights.expand_as(input)\n",
        "\n",
        "        # create default class_weights if None\n",
        "        if self.class_weights is None:\n",
        "            class_weights = torch.ones(input.size()[1]).float().to(input.device)\n",
        "        else:\n",
        "            class_weights = self.class_weights\n",
        "\n",
        "        # resize class_weights to be broadcastable into the weights\n",
        "        class_weights = class_weights.view(1, -1, 1, 1, 1)\n",
        "\n",
        "        # multiply weights tensor by class weights\n",
        "        weights = class_weights * weights\n",
        "\n",
        "        # compute the losses\n",
        "        result = -weights * target * log_probabilities\n",
        "        # average the losses\n",
        "        return result.mean()\n",
        "\n",
        "\n",
        "class WeightedSmoothL1Loss(nn.SmoothL1Loss):\n",
        "    def __init__(self, threshold, initial_weight, apply_below_threshold=True):\n",
        "        super().__init__(reduction=\"none\")\n",
        "        self.threshold = threshold\n",
        "        self.apply_below_threshold = apply_below_threshold\n",
        "        self.weight = initial_weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = super().forward(input, target)\n",
        "\n",
        "        if self.apply_below_threshold:\n",
        "            mask = target < self.threshold\n",
        "        else:\n",
        "            mask = target >= self.threshold\n",
        "\n",
        "        l1[mask] = l1[mask] * self.weight\n",
        "\n",
        "        return l1.mean()\n",
        "\n",
        "\n",
        "def flatten(tensor):\n",
        "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
        "    The shapes are transformed as follows:\n",
        "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
        "    \"\"\"\n",
        "    # number of channels\n",
        "    C = tensor.size(1)\n",
        "    # new axis order\n",
        "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
        "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
        "    transposed = tensor.permute(axis_order)\n",
        "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
        "    return transposed.contiguous().view(C, -1)\n",
        "\n",
        "\n",
        "def get_loss_criterion(config):\n",
        "    \"\"\"\n",
        "    Returns the loss function based on provided configuration\n",
        "    :param config: (dict) a top level configuration object containing the 'loss' key\n",
        "    :return: an instance of the loss function\n",
        "    \"\"\"\n",
        "    assert 'loss' in config, 'Could not find loss function configuration'\n",
        "    loss_config = config['loss']\n",
        "    name = loss_config.pop('name')\n",
        "\n",
        "    ignore_index = loss_config.pop('ignore_index', None)\n",
        "    skip_last_target = loss_config.pop('skip_last_target', False)\n",
        "    weight = loss_config.pop('weight', None)\n",
        "\n",
        "    if weight is not None:\n",
        "        # convert to cuda tensor if necessary\n",
        "        weight = torch.tensor(weight).to(config['device'])\n",
        "\n",
        "    pos_weight = loss_config.pop('pos_weight', None)\n",
        "    if pos_weight is not None:\n",
        "        # convert to cuda tensor if necessary\n",
        "        pos_weight = torch.tensor(pos_weight).to(config['device'])\n",
        "\n",
        "    loss = _create_loss(name, loss_config, weight, ignore_index, pos_weight)\n",
        "\n",
        "    if not (ignore_index is None or name in ['CrossEntropyLoss', 'WeightedCrossEntropyLoss']):\n",
        "        # use MaskingLossWrapper only for non-cross-entropy losses, since CE losses allow specifying 'ignore_index' directly\n",
        "        loss = _MaskingLossWrapper(loss, ignore_index)\n",
        "\n",
        "    if skip_last_target:\n",
        "        loss = SkipLastTargetChannelWrapper(loss, loss_config.get('squeeze_channel', False))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "#######################################################################################################################\n",
        "\n",
        "def create_loss(name, loss_config, weight, ignore_index, pos_weight):\n",
        "    if name == 'BCEWithLogitsLoss':\n",
        "        return nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    elif name == 'BCEDiceLoss':\n",
        "        alpha = loss_config.alpha\n",
        "        beta = loss_config.beta\n",
        "        return BCEDiceLoss(alpha, beta) \n",
        "    elif name == 'CrossEntropyLoss':\n",
        "        if ignore_index is None:\n",
        "            ignore_index = -100  # use the default 'ignore_index' as defined in the CrossEntropyLoss\n",
        "        return nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n",
        "    elif name == 'WeightedCrossEntropyLoss':\n",
        "        if ignore_index is None:\n",
        "            ignore_index = -100  # use the default 'ignore_index' as defined in the CrossEntropyLoss\n",
        "        return WeightedCrossEntropyLoss(ignore_index=ignore_index)\n",
        "    elif name == 'PixelWiseCrossEntropyLoss':\n",
        "        return PixelWiseCrossEntropyLoss(class_weights=weight, ignore_index=ignore_index)\n",
        "    elif name == 'GeneralizedDiceLoss':\n",
        "        normalization = loss_config.get('normalization', 'sigmoid')\n",
        "        return GeneralizedDiceLoss(normalization=normalization)\n",
        "    elif name == 'DiceLoss':\n",
        "        normalization = loss_config.get('normalization', 'sigmoid')\n",
        "        return DiceLoss(weight=weight, normalization=normalization)\n",
        "    elif name == 'MSELoss':\n",
        "        return MSELoss()\n",
        "    elif name == 'SmoothL1Loss':\n",
        "        return SmoothL1Loss()\n",
        "    elif name == 'L1Loss':\n",
        "        return L1Loss()\n",
        "    elif name == 'WeightedSmoothL1Loss':\n",
        "        return WeightedSmoothL1Loss(threshold=loss_config['threshold'],\n",
        "                                    initial_weight=loss_config['initial_weight'],\n",
        "                                    apply_below_threshold=loss_config.get('apply_below_threshold', True))\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unsupported loss function: '{name}'\")\n"
      ],
      "metadata": {
        "id": "6tU5l8Qn7hai"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ],
      "metadata": {
        "id": "FI847wXY7qAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from skimage import measure\n",
        "from skimage.metrics import adapted_rand_error, peak_signal_noise_ratio\n",
        "\n",
        "#from losses import compute_per_channel_dice\n",
        "#from seg_metrics import AveragePrecision, Accuracy\n",
        "#from utils import get_logger, expand_as_one_hot, convert_to_numpy\n",
        "\n",
        "logger = get_logger('EvalMetric')\n",
        "\n",
        "\n",
        "class DiceCoefficient:\n",
        "    \"\"\"Computes Dice Coefficient.\n",
        "    Generalized to multiple channels by computing per-channel Dice Score\n",
        "    (as described in https://arxiv.org/pdf/1707.03237.pdf) and theTn simply taking the average.\n",
        "    Input is expected to be probabilities instead of logits.\n",
        "    This metric is mostly useful when channels contain the same semantic class (e.g. affinities computed with different offsets).\n",
        "    DO NOT USE this metric when training with DiceLoss, otherwise the results will be biased towards the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=1e-6, normalization='sigmoid', **kwargs):\n",
        "        self.epsilon = epsilon\n",
        "        assert normalization in ['sigmoid', 'softmax', 'none']\n",
        "        if normalization == 'sigmoid':\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        elif normalization == 'softmax':\n",
        "            self.normalization = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            self.normalization = lambda x: x\n",
        "    def __call__(self, input, target):\n",
        "        # Average across channels in order to get the final score\n",
        "        input = self.normalization(input)\n",
        "        dice = compute_per_channel_dice(input, target, epsilon=self.epsilon)\n",
        "\n",
        "        return torch.mean(dice)\n",
        "\n",
        "\n",
        "class MeanIoU:\n",
        "    \"\"\"\n",
        "    Computes IoU for each class separately and then averages over all classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, skip_channels=(), ignore_index=None, **kwargs):\n",
        "        \"\"\"\n",
        "        :param skip_channels: list/tuple of channels to be ignored from the IoU computation\n",
        "        :param ignore_index: id of the label to be ignored from IoU computation\n",
        "        \"\"\"\n",
        "        self.ignore_index = ignore_index\n",
        "        self.skip_channels = skip_channels\n",
        "\n",
        "    def __call__(self, input, target):\n",
        "        \"\"\"\n",
        "        :param input: 5D probability maps torch float tensor (NxCxDxHxW)\n",
        "        :param target: 4D or 5D ground truth torch tensor. 4D (NxDxHxW) tensor will be expanded to 5D as one-hot\n",
        "        :return: intersection over union averaged over all channels\n",
        "        \"\"\"\n",
        "        assert input.dim() == 5\n",
        "\n",
        "        n_classes = input.size()[1]\n",
        "\n",
        "        if target.dim() == 4:\n",
        "            target = expand_as_one_hot(target, C=n_classes, ignore_index=self.ignore_index)\n",
        "\n",
        "        assert input.size() == target.size()\n",
        "\n",
        "        per_batch_iou = []\n",
        "        for _input, _target in zip(input, target):\n",
        "            binary_prediction = self._binarize_predictions(_input, n_classes)\n",
        "\n",
        "            if self.ignore_index is not None:\n",
        "                # zero out ignore_index\n",
        "                mask = _target == self.ignore_index\n",
        "                binary_prediction[mask] = 0\n",
        "                _target[mask] = 0\n",
        "\n",
        "            # convert to uint8 just in case\n",
        "            binary_prediction = binary_prediction.byte()\n",
        "            _target = _target.byte()\n",
        "\n",
        "            per_channel_iou = []\n",
        "            for c in range(n_classes):\n",
        "                if c in self.skip_channels:\n",
        "                    continue\n",
        "\n",
        "                per_channel_iou.append(self._jaccard_index(binary_prediction[c], _target[c]))\n",
        "\n",
        "            assert per_channel_iou, \"All channels were ignored from the computation\"\n",
        "            mean_iou = torch.mean(torch.tensor(per_channel_iou))\n",
        "            per_batch_iou.append(mean_iou)\n",
        "\n",
        "        return torch.mean(torch.tensor(per_batch_iou))\n",
        "\n",
        "    def _binarize_predictions(self, input, n_classes):\n",
        "        \"\"\"\n",
        "        Puts 1 for the class/channel with the highest probability and 0 in other channels. Returns byte tensor of the\n",
        "        same size as the input tensor.\n",
        "        \"\"\"\n",
        "        if n_classes == 1:\n",
        "            # for single channel input just threshold the probability map\n",
        "            result = input > 0.5\n",
        "            return result.long()\n",
        "\n",
        "        _, max_index = torch.max(input, dim=0, keepdim=True)\n",
        "        return torch.zeros_like(input, dtype=torch.uint8).scatter_(0, max_index, 1)\n",
        "\n",
        "    def _jaccard_index(self, prediction, target):\n",
        "        \"\"\"\n",
        "        Computes IoU for a given target and prediction tensors\n",
        "        \"\"\"\n",
        "        return torch.sum(prediction & target).float() / torch.clamp(torch.sum(prediction | target).float(), min=1e-8)\n",
        "\n",
        "\n",
        "class AdaptedRandError:\n",
        "    \"\"\"\n",
        "    A functor which computes an Adapted Rand error as defined by the SNEMI3D contest\n",
        "    (http://brainiac2.mit.edu/SNEMI3D/evaluation).\n",
        "\n",
        "    This is a generic implementation which takes the input, converts it to the segmentation image (see `input_to_segm()`)\n",
        "    and then computes the ARand between the segmentation and the ground truth target. Depending on one's use case\n",
        "    it's enough to extend this class and implement the `input_to_segm` method.\n",
        "\n",
        "    Args:\n",
        "        use_last_target (bool): use only the last channel from the target to compute the ARand\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_last_target=False, ignore_index=None, **kwargs):\n",
        "        self.use_last_target = use_last_target\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def __call__(self, input, target):\n",
        "        \"\"\"\n",
        "        Compute ARand Error for each input, target pair in the batch and return the mean value.\n",
        "\n",
        "        Args:\n",
        "            input (torch.tensor): 5D (NCDHW) output from the network\n",
        "            target (torch.tensor): 4D (NDHW) ground truth segmentation\n",
        "\n",
        "        Returns:\n",
        "            average ARand Error across the batch\n",
        "        \"\"\"\n",
        "\n",
        "        def _arand_err(gt, seg):\n",
        "            n_seg = len(np.unique(seg))\n",
        "            if n_seg == 1:\n",
        "                return 0.\n",
        "            return adapted_rand_error(gt, seg)[0]\n",
        "\n",
        "        # converts input and target to numpy arrays\n",
        "        input, target = convert_to_numpy(input, target)\n",
        "        if self.use_last_target:\n",
        "            target = target[:, -1, ...]  # 4D\n",
        "        else:\n",
        "            # use 1st target channel\n",
        "            target = target[:, 0, ...]  # 4D\n",
        "\n",
        "        # ensure target is of integer type\n",
        "        target = target.astype(np.int)\n",
        "\n",
        "        if self.ignore_index is not None:\n",
        "            target[target == self.ignore_index] = 0\n",
        "\n",
        "        per_batch_arand = []\n",
        "        for _input, _target in zip(input, target):\n",
        "            n_clusters = len(np.unique(_target))\n",
        "            # skip ARand eval if there is only one label in the patch due to the zero-division error in Arand impl\n",
        "            # xxx/skimage/metrics/_adapted_rand_error.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
        "            # precision = sum_p_ij2 / sum_a2\n",
        "            logger.info(f'Number of ground truth clusters: {n_clusters}')\n",
        "            if n_clusters == 1:\n",
        "                logger.info('Skipping ARandError computation: only 1 label present in the ground truth')\n",
        "                per_batch_arand.append(0.)\n",
        "                continue\n",
        "\n",
        "            # convert _input to segmentation CDHW\n",
        "            segm = self.input_to_segm(_input)\n",
        "            assert segm.ndim == 4\n",
        "\n",
        "            # compute per channel arand and return the minimum value\n",
        "            per_channel_arand = [_arand_err(_target, channel_segm) for channel_segm in segm]\n",
        "            logger.info(f'Min ARand for channel: {np.argmin(per_channel_arand)}')\n",
        "            per_batch_arand.append(np.min(per_channel_arand))\n",
        "\n",
        "        # return mean arand error\n",
        "        mean_arand = torch.mean(torch.tensor(per_batch_arand))\n",
        "        logger.info(f'ARand: {mean_arand.item()}')\n",
        "        return mean_arand\n",
        "\n",
        "    def input_to_segm(self, input):\n",
        "        \"\"\"\n",
        "        Converts input tensor (output from the network) to the segmentation image. E.g. if the input is the boundary\n",
        "        pmaps then one option would be to threshold it and run connected components in order to return the segmentation.\n",
        "\n",
        "        :param input: 4D tensor (CDHW)\n",
        "        :return: segmentation volume either 4D (segmentation per channel)\n",
        "        \"\"\"\n",
        "        # by deafult assume that input is a segmentation volume itself\n",
        "        return input\n",
        "\n",
        "\n",
        "class BoundaryAdaptedRandError(AdaptedRandError):\n",
        "    \"\"\"\n",
        "    Compute ARand between the input boundary map and target segmentation.\n",
        "    Boundary map is thresholded, and connected components is run to get the predicted segmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, thresholds=None, use_last_target=True, ignore_index=None, input_channel=None, invert_pmaps=True,\n",
        "                 save_plots=False, plots_dir='.', **kwargs):\n",
        "        super().__init__(use_last_target=use_last_target, ignore_index=ignore_index, save_plots=save_plots,\n",
        "                         plots_dir=plots_dir, **kwargs)\n",
        "\n",
        "        if thresholds is None:\n",
        "            thresholds = [0.3, 0.4, 0.5, 0.6]\n",
        "        assert isinstance(thresholds, list)\n",
        "        self.thresholds = thresholds\n",
        "        self.input_channel = input_channel\n",
        "        self.invert_pmaps = invert_pmaps\n",
        "\n",
        "    def input_to_segm(self, input):\n",
        "        if self.input_channel is not None:\n",
        "            input = np.expand_dims(input[self.input_channel], axis=0)\n",
        "\n",
        "        segs = []\n",
        "        for predictions in input:\n",
        "            for th in self.thresholds:\n",
        "                # threshold probability maps\n",
        "                predictions = predictions > th\n",
        "\n",
        "                if self.invert_pmaps:\n",
        "                    # for connected component analysis we need to treat boundary signal as background\n",
        "                    # assign 0-label to boundary mask\n",
        "                    predictions = np.logical_not(predictions)\n",
        "\n",
        "                predictions = predictions.astype(np.uint8)\n",
        "                # run connected components on the predicted mask; consider only 1-connectivity\n",
        "                seg = measure.label(predictions, background=0, connectivity=1)\n",
        "                segs.append(seg)\n",
        "\n",
        "        return np.stack(segs)\n",
        "\n",
        "\n",
        "class GenericAdaptedRandError(AdaptedRandError):\n",
        "    def __init__(self, input_channels, thresholds=None, use_last_target=True, ignore_index=None, invert_channels=None,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__(use_last_target=use_last_target, ignore_index=ignore_index, **kwargs)\n",
        "        assert isinstance(input_channels, list) or isinstance(input_channels, tuple)\n",
        "        self.input_channels = input_channels\n",
        "        if thresholds is None:\n",
        "            thresholds = [0.3, 0.4, 0.5, 0.6]\n",
        "        assert isinstance(thresholds, list)\n",
        "        self.thresholds = thresholds\n",
        "        if invert_channels is None:\n",
        "            invert_channels = []\n",
        "        self.invert_channels = invert_channels\n",
        "\n",
        "    def input_to_segm(self, input):\n",
        "        # pick only the channels specified in the input_channels\n",
        "        results = []\n",
        "        for i in self.input_channels:\n",
        "            c = input[i]\n",
        "            # invert channel if necessary\n",
        "            if i in self.invert_channels:\n",
        "                c = 1 - c\n",
        "            results.append(c)\n",
        "\n",
        "        input = np.stack(results)\n",
        "\n",
        "        segs = []\n",
        "        for predictions in input:\n",
        "            for th in self.thresholds:\n",
        "                # run connected components on the predicted mask; consider only 1-connectivity\n",
        "                seg = measure.label((predictions > th).astype(np.uint8), background=0, connectivity=1)\n",
        "                segs.append(seg)\n",
        "\n",
        "        return np.stack(segs)\n",
        "\n",
        "\n",
        "class GenericAveragePrecision:\n",
        "    def __init__(self, min_instance_size=None, use_last_target=False, metric='ap', **kwargs):\n",
        "        self.min_instance_size = min_instance_size\n",
        "        self.use_last_target = use_last_target\n",
        "        assert metric in ['ap', 'acc']\n",
        "        if metric == 'ap':\n",
        "            # use AveragePrecision\n",
        "            self.metric = AveragePrecision()\n",
        "        else:\n",
        "            # use Accuracy at 0.5 IoU\n",
        "            self.metric = Accuracy(iou_threshold=0.5)\n",
        "\n",
        "    def __call__(self, input, target):\n",
        "        if target.dim() == 5:\n",
        "            if self.use_last_target:\n",
        "                target = target[:, -1, ...]  # 4D\n",
        "            else:\n",
        "                # use 1st target channel\n",
        "                target = target[:, 0, ...]  # 4D\n",
        "\n",
        "        input1 = input2 = input\n",
        "        multi_head = isinstance(input, tuple)\n",
        "        if multi_head:\n",
        "            input1, input2 = input\n",
        "\n",
        "        input1, input2, target = convert_to_numpy(input1, input2, target)\n",
        "\n",
        "        batch_aps = []\n",
        "        i_batch = 0\n",
        "        # iterate over the batch\n",
        "        for inp1, inp2, tar in zip(input1, input2, target):\n",
        "            if multi_head:\n",
        "                inp = (inp1, inp2)\n",
        "            else:\n",
        "                inp = inp1\n",
        "\n",
        "            segs = self.input_to_seg(inp, tar)  # expects 4D\n",
        "            assert segs.ndim == 4\n",
        "            # convert target to seg\n",
        "            tar = self.target_to_seg(tar)\n",
        "\n",
        "            # filter small instances if necessary\n",
        "            tar = self._filter_instances(tar)\n",
        "\n",
        "            # compute average precision per channel\n",
        "            segs_aps = [self.metric(self._filter_instances(seg), tar) for seg in segs]\n",
        "\n",
        "            logger.info(f'Batch: {i_batch}. Max Average Precision for channel: {np.argmax(segs_aps)}')\n",
        "            # save max AP\n",
        "            batch_aps.append(np.max(segs_aps))\n",
        "            i_batch += 1\n",
        "\n",
        "        return torch.tensor(batch_aps).mean()\n",
        "\n",
        "    def _filter_instances(self, input):\n",
        "        \"\"\"\n",
        "        Filters instances smaller than 'min_instance_size' by overriding them with 0-index\n",
        "        :param input: input instance segmentation\n",
        "        \"\"\"\n",
        "        if self.min_instance_size is not None:\n",
        "            labels, counts = np.unique(input, return_counts=True)\n",
        "            for label, count in zip(labels, counts):\n",
        "                if count < self.min_instance_size:\n",
        "                    input[input == label] = 0\n",
        "        return input\n",
        "\n",
        "    def input_to_seg(self, input, target=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def target_to_seg(self, target):\n",
        "        return target\n",
        "\n",
        "\n",
        "class BlobsAveragePrecision(GenericAveragePrecision):\n",
        "    \"\"\"\n",
        "    Computes Average Precision given foreground prediction and ground truth instance segmentation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, thresholds=None, metric='ap', min_instance_size=None, input_channel=0, **kwargs):\n",
        "        super().__init__(min_instance_size=min_instance_size, use_last_target=True, metric=metric)\n",
        "        if thresholds is None:\n",
        "            thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "        assert isinstance(thresholds, list)\n",
        "        self.thresholds = thresholds\n",
        "        self.input_channel = input_channel\n",
        "\n",
        "    def input_to_seg(self, input, target=None):\n",
        "        input = input[self.input_channel]\n",
        "        segs = []\n",
        "        for th in self.thresholds:\n",
        "            # threshold and run connected components\n",
        "            mask = (input > th).astype(np.uint8)\n",
        "            seg = measure.label(mask, background=0, connectivity=1)\n",
        "            segs.append(seg)\n",
        "        return np.stack(segs)\n",
        "\n",
        "\n",
        "class BlobsBoundaryAveragePrecision(GenericAveragePrecision):\n",
        "    \"\"\"\n",
        "    Computes Average Precision given foreground prediction, boundary prediction and ground truth instance segmentation.\n",
        "    Segmentation mask is computed as (P_mask - P_boundary) > th followed by a connected component\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, thresholds=None, metric='ap', min_instance_size=None, **kwargs):\n",
        "        super().__init__(min_instance_size=min_instance_size, use_last_target=True, metric=metric)\n",
        "        if thresholds is None:\n",
        "            thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "        assert isinstance(thresholds, list)\n",
        "        self.thresholds = thresholds\n",
        "\n",
        "    def input_to_seg(self, input, target=None):\n",
        "        # input = P_mask - P_boundary\n",
        "        input = input[0] - input[1]\n",
        "        segs = []\n",
        "        for th in self.thresholds:\n",
        "            # threshold and run connected components\n",
        "            mask = (input > th).astype(np.uint8)\n",
        "            seg = measure.label(mask, background=0, connectivity=1)\n",
        "            segs.append(seg)\n",
        "        return np.stack(segs)\n",
        "\n",
        "\n",
        "class BoundaryAveragePrecision(GenericAveragePrecision):\n",
        "    \"\"\"\n",
        "    Computes Average Precision given boundary prediction and ground truth instance segmentation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, thresholds=None, min_instance_size=None, input_channel=0, **kwargs):\n",
        "        super().__init__(min_instance_size=min_instance_size, use_last_target=True)\n",
        "        if thresholds is None:\n",
        "            thresholds = [0.3, 0.4, 0.5, 0.6]\n",
        "        assert isinstance(thresholds, list)\n",
        "        self.thresholds = thresholds\n",
        "        self.input_channel = input_channel\n",
        "\n",
        "    def input_to_seg(self, input, target=None):\n",
        "        input = input[self.input_channel]\n",
        "        segs = []\n",
        "        for th in self.thresholds:\n",
        "            seg = measure.label(np.logical_not(input > th).astype(np.uint8), background=0, connectivity=1)\n",
        "            segs.append(seg)\n",
        "        return np.stack(segs)\n",
        "\n",
        "\n",
        "class PSNR:\n",
        "    \"\"\"\n",
        "    Computes Peak Signal to Noise Ratio. Use e.g. as an eval metric for denoising task\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, input, target):\n",
        "        input, target = convert_to_numpy(input, target)\n",
        "        return peak_signal_noise_ratio(target, input)\n",
        "\n",
        "\n",
        "def get_evaluation_metric(config):\n",
        "    \"\"\"\n",
        "    Returns the evaluation metric function based on provided configuration\n",
        "    :param config: (dict) a top level configuration object containing the 'eval_metric' key\n",
        "    :return: an instance of the evaluation metric\n",
        "    \"\"\"\n",
        "\n",
        "    def _metric_class(class_name):\n",
        "        m = importlib.import_module('pytorch3dunet.unet3d.metrics')\n",
        "        clazz = getattr(m, class_name)\n",
        "        return clazz\n",
        "\n",
        "    assert 'eval_metric' in config, 'Could not find evaluation metric configuration'\n",
        "    metric_config = config['eval_metric']\n",
        "    metric_class = _metric_class(metric_config['name'])\n",
        "    return metric_class(**metric_config)\n"
      ],
      "metadata": {
        "id": "e67uRfne7z4V"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "_d02eE8H764N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "#from buildingblocks import *\n",
        "#from utils import number_of_features_per_level\n",
        "\n",
        "class Abstract3DUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for standard and residual UNet.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output segmentation masks;\n",
        "            Note that that the of out_channels might correspond to either\n",
        "            different semantic classes or to different binary segmentation mask.\n",
        "            It's up to the user of the class to interpret the out_channels and\n",
        "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
        "            or BCEWithLogitsLoss (two-class) respectively)\n",
        "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
        "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
        "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
        "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
        "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
        "        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n",
        "        layer_order (string): determines the order of layers\n",
        "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
        "            See `SingleConv` for more info\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
        "        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n",
        "            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n",
        "        testing (bool): if True (testing mode) the `final_activation` (if present, i.e. `is_segmentation=true`)\n",
        "            will be applied as the last operation during the forward pass; if False the model is in training mode\n",
        "            and the `final_activation` (even if present) won't be applied; default: False\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
        "        pool_kernel_size (int or tuple): the size of the window\n",
        "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, testing=False,\n",
        "                 conv_kernel_size=3, pool_kernel_size=2, conv_padding=1, **kwargs):\n",
        "        super(Abstract3DUNet, self).__init__()\n",
        "\n",
        "        self.testing = testing\n",
        "\n",
        "        if isinstance(f_maps, int):\n",
        "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
        "\n",
        "        assert isinstance(f_maps, list) or isinstance(f_maps, tuple)\n",
        "        assert len(f_maps) > 1, \"Required at least 2 levels in the U-Net\"\n",
        "\n",
        "        # create encoder path\n",
        "        self.encoders = create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order,\n",
        "                                        num_groups, pool_kernel_size)\n",
        "\n",
        "        # create decoder path\n",
        "        self.decoders = create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
        "                                        upsample=True)\n",
        "\n",
        "        # in the last layer a 11 convolution reduces the number of output\n",
        "        # channels to the number of labels\n",
        "        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
        "\n",
        "        if is_segmentation:\n",
        "            # semantic segmentation problem\n",
        "            if final_sigmoid:\n",
        "                self.final_activation = nn.Sigmoid()\n",
        "            else:\n",
        "                self.final_activation = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            # regression problem\n",
        "            self.final_activation = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder part\n",
        "        encoders_features = []\n",
        "        #print(\"Encoder part\")\n",
        "        for encoder in self.encoders:\n",
        "            #print(x.shape)\n",
        "            x = encoder(x)\n",
        "            # reverse the encoder outputs to be aligned with the decoder\n",
        "            encoders_features.insert(0, x)\n",
        "\n",
        "        # remove the last encoder's output from the list\n",
        "        # !!remember: it's the 1st in the list\n",
        "        encoders_features = encoders_features[1:]\n",
        "        #print()\n",
        "        # decoder part\n",
        "        #print(\"Decoder part\")\n",
        "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
        "            #print(encoder_features.shape , x.shape) \n",
        "            # pass the output from the corresponding encoder and the output\n",
        "            # of the previous decoder\n",
        "            x = decoder(encoder_features, x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs\n",
        "        # logits and it's up to the user to normalize it before visualising with tensorboard or computing validation metric\n",
        "        if self.testing and self.final_activation is not None:\n",
        "            x = self.final_activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet3D(Abstract3DUNet):\n",
        "    \"\"\"\n",
        "    3DUnet model from\n",
        "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
        "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
        "\n",
        "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=16, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
        "        super(UNet3D, self).__init__(in_channels=in_channels,\n",
        "                                     out_channels=out_channels,\n",
        "                                     final_sigmoid=final_sigmoid,\n",
        "                                     basic_module=DoubleConv,\n",
        "                                     f_maps=f_maps,\n",
        "                                     layer_order=layer_order,\n",
        "                                     num_groups=num_groups,\n",
        "                                     num_levels=num_levels,\n",
        "                                     is_segmentation=is_segmentation,\n",
        "                                     conv_padding=conv_padding,\n",
        "                                     **kwargs)\n",
        "\n",
        "\n",
        "class ResidualUNet3D(Abstract3DUNet):\n",
        "    \"\"\"\n",
        "    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n",
        "    Uses ExtResNetBlock as a basic building block, summation joining instead\n",
        "    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).\n",
        "    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=16, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
        "        super(ResidualUNet3D, self).__init__(in_channels=in_channels,\n",
        "                                             out_channels=out_channels,\n",
        "                                             final_sigmoid=final_sigmoid,\n",
        "                                             basic_module=ExtResNetBlock,\n",
        "                                             f_maps=f_maps,\n",
        "                                             layer_order=layer_order,\n",
        "                                             num_groups=num_groups,\n",
        "                                             num_levels=num_levels,\n",
        "                                             is_segmentation=is_segmentation,\n",
        "                                             conv_padding=conv_padding,\n",
        "                                             **kwargs)"
      ],
      "metadata": {
        "id": "a_rsxDmO79Jd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seg_metrics"
      ],
      "metadata": {
        "id": "3AFCkFud7_Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import contingency_table\n",
        "\n",
        "\n",
        "def precision(tp, fp, fn):\n",
        "    return tp / (tp + fp) if tp > 0 else 0\n",
        "\n",
        "\n",
        "def recall(tp, fp, fn):\n",
        "    return tp / (tp + fn) if tp > 0 else 0\n",
        "\n",
        "\n",
        "def accuracy(tp, fp, fn):\n",
        "    return tp / (tp + fp + fn) if tp > 0 else 0\n",
        "\n",
        "\n",
        "def f1(tp, fp, fn):\n",
        "    return (2 * tp) / (2 * tp + fp + fn) if tp > 0 else 0\n",
        "\n",
        "\n",
        "def _relabel(input):\n",
        "    _, unique_labels = np.unique(input, return_inverse=True)\n",
        "    return unique_labels.reshape(input.shape)\n",
        "\n",
        "\n",
        "def _iou_matrix(gt, seg):\n",
        "    # relabel gt and seg for smaller memory footprint of contingency table\n",
        "    gt = _relabel(gt)\n",
        "    seg = _relabel(seg)\n",
        "\n",
        "    # get number of overlapping pixels between GT and SEG\n",
        "    n_inter = contingency_table(gt, seg).A\n",
        "\n",
        "    # number of pixels for GT instances\n",
        "    n_gt = n_inter.sum(axis=1, keepdims=True)\n",
        "    # number of pixels for SEG instances\n",
        "    n_seg = n_inter.sum(axis=0, keepdims=True)\n",
        "\n",
        "    # number of pixels in the union between GT and SEG instances\n",
        "    n_union = n_gt + n_seg - n_inter\n",
        "\n",
        "    iou_matrix = n_inter / n_union\n",
        "    # make sure that the values are within [0,1] range\n",
        "    assert 0 <= np.min(iou_matrix) <= np.max(iou_matrix) <= 1\n",
        "\n",
        "    return iou_matrix\n",
        "\n",
        "\n",
        "class SegmentationMetrics:\n",
        "    \"\"\"\n",
        "    Computes precision, recall, accuracy, f1 score for a given ground truth and predicted segmentation.\n",
        "    Contingency table for a given ground truth and predicted segmentation is computed eagerly upon construction\n",
        "    of the instance of `SegmentationMetrics`.\n",
        "\n",
        "    Args:\n",
        "        gt (ndarray): ground truth segmentation\n",
        "        seg (ndarray): predicted segmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gt, seg):\n",
        "        self.iou_matrix = _iou_matrix(gt, seg)\n",
        "\n",
        "    def metrics(self, iou_threshold):\n",
        "        \"\"\"\n",
        "        Computes precision, recall, accuracy, f1 score at a given IoU threshold\n",
        "        \"\"\"\n",
        "        # ignore background\n",
        "        iou_matrix = self.iou_matrix[1:, 1:]\n",
        "        detection_matrix = (iou_matrix > iou_threshold).astype(np.uint8)\n",
        "        n_gt, n_seg = detection_matrix.shape\n",
        "\n",
        "        # if the iou_matrix is empty or all values are 0\n",
        "        trivial = min(n_gt, n_seg) == 0 or np.all(detection_matrix == 0)\n",
        "        if trivial:\n",
        "            tp = fp = fn = 0\n",
        "        else:\n",
        "            # count non-zero rows to get the number of TP\n",
        "            tp = np.count_nonzero(detection_matrix.sum(axis=1))\n",
        "            # count zero rows to get the number of FN\n",
        "            fn = n_gt - tp\n",
        "            # count zero columns to get the number of FP\n",
        "            fp = n_seg - np.count_nonzero(detection_matrix.sum(axis=0))\n",
        "\n",
        "        return {\n",
        "            'precision': precision(tp, fp, fn),\n",
        "            'recall': recall(tp, fp, fn),\n",
        "            'accuracy': accuracy(tp, fp, fn),\n",
        "            'f1': f1(tp, fp, fn)\n",
        "        }\n",
        "\n",
        "\n",
        "class Accuracy:\n",
        "    \"\"\"\n",
        "    Computes accuracy between ground truth and predicted segmentation a a given threshold value.\n",
        "    Defined as: AC = TP / (TP + FP + FN).\n",
        "    Kaggle DSB2018 calls it Precision, see:\n",
        "    https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, iou_threshold):\n",
        "        self.iou_threshold = iou_threshold\n",
        "\n",
        "    def __call__(self, input_seg, gt_seg):\n",
        "        metrics = SegmentationMetrics(gt_seg, input_seg).metrics(self.iou_threshold)\n",
        "        return metrics['accuracy']\n",
        "\n",
        "\n",
        "class AveragePrecision:\n",
        "    \"\"\"\n",
        "    Average precision taken for the IoU range (0.5, 0.95) with a step of 0.05 as defined in:\n",
        "    https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.iou_range = np.linspace(0.50, 0.95, 10)\n",
        "\n",
        "    def __call__(self, input_seg, gt_seg):\n",
        "        # compute contingency_table\n",
        "        sm = SegmentationMetrics(gt_seg, input_seg)\n",
        "        # compute accuracy for each threshold\n",
        "        acc = [sm.metrics(iou)['accuracy'] for iou in self.iou_range]\n",
        "        # return the average\n",
        "        return np.mean(acc)\n"
      ],
      "metadata": {
        "id": "9aZURnVY8Bn0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainer"
      ],
      "metadata": {
        "id": "7QHWf49R8HeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model , data_loaders, num_epochs , optimizer , scheduler ,criterion , eval_metric):\n",
        "\n",
        "    best_dice = 0\n",
        "    best_epoch = 0\n",
        "    train_result = []\n",
        "    val_result = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train','val']:\n",
        "            if phase == \"train\":\n",
        "                model.train() # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_dice1 = 0.0\n",
        "            running_dice2 = 0.0\n",
        "            running_dice3 = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for batch in data_loaders[phase]:\n",
        "                cine_seri , mask , flow = batch[\"image\"].float().cuda() , batch[\"label\"].cuda() , batch[\"flow\"].float().cuda()\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    logit_map = model(cine_seri , flow)\n",
        "                    #print(\"logit_map\" , logit_map.shape)\n",
        "                    loss = criterion(logit_map.float() , mask.float())\n",
        "                    dice_score1 , dice_score2  = eval_metric(logit_map.float() , mask.float())\n",
        "                    # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()   \n",
        "                \n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * cine_seri.shape[0]\n",
        "                running_dice1 += dice_score1.item()* cine_seri.shape[0]\n",
        "                running_dice2 += dice_score2.item()* cine_seri.shape[0]\n",
        "\n",
        "            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n",
        "            epoch_dice1 = running_dice1 / len(data_loaders[phase].dataset)\n",
        "            epoch_dice2 = running_dice2 / len(data_loaders[phase].dataset)\n",
        "\n",
        "            \n",
        "            \n",
        "            torch.save(model.state_dict(), \"last.pth\")\n",
        "\n",
        "            \n",
        "            if phase == \"train\":\n",
        "                train_result.append(epoch_dice1)\n",
        "            \n",
        "            if phase == \"val\":\n",
        "                val_result.append(epoch_dice1)\n",
        "                scheduler.step(epoch_loss)\n",
        "\n",
        "                \n",
        "            if phase == \"val\" and (epoch_dice1) > best_dice:\n",
        "                best_dice = epoch_dice1\n",
        "                best_model = model\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"best.pth\")\n",
        "            \n",
        "            print('{} ,Loss: {:.4f} , Dice1: {:.4f} ,Dice2: {:.4f} , Best Dice: {:.4f} , Best Epoch: {:.4f}'.format(phase, epoch_loss , epoch_dice1 ,epoch_dice2  , best_dice , best_epoch) )\n",
        "\n",
        "    best_model_name = \"best_\" + str(best_dice) + \".pth\"\n",
        "    torch.save(best_model.state_dict(), best_model_name)  \n",
        "    print(\"Best Dice:\" , best_dice)         \n",
        "    return model , train_result , val_result"
      ],
      "metadata": {
        "id": "R4WL0aIo8KQG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "aaTf2t-r8bmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "import glob"
      ],
      "metadata": {
        "id": "H5n9L89LBCeE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataloader"
      ],
      "metadata": {
        "id": "Ce2nSXjcCl98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomDataset(Dataset):\n",
        "    def __init__(self, I_path, L_path,k):\n",
        "        N = len(I_path)\n",
        "        # Empty list\n",
        "        self.images_list = []\n",
        "        self.label_list = []\n",
        "        #for i in range(N):   # We should have this but we will use the first five dataset only for memory shortage\n",
        "        for i in range(5):\n",
        "            data_char = layer_properties[i] # Loading data characteristics\n",
        "            first_layer = data_char[\"first_layer\"] # Loading the first cancer layer number\n",
        "            last_layer = data_char[\"last_layer\"] # Loading the last cancer layer number\n",
        "            image_data = np.asarray(nib.load(I_path[i]).dataobj) # Convert the .nii object to array\n",
        "            label_data = np.asarray(nib.load(L_path[i]).dataobj) # Convert the .nii object to array\n",
        "            for j in range(first_layer,last_layer+1-k,k): # Add each slice of layers with K width to the dataset\n",
        "                image = image_data[:,:,first_layer:first_layer+k] # Slice  the image\n",
        "                image = image.reshape((1,k,512,512)) # Convert from (512,512,k) --> (1,k,512,512)\n",
        "                label = label_data[:,:,first_layer:first_layer+k] # Slice  the label\n",
        "                label = label.reshape((1,k,512,512)) # Convert from (512,512,k) --> (1,k,512,512)\n",
        "                self.images_list.append(image) # Add to the list(Dataset) \n",
        "                self.label_list.append(label) # Add to the list(Dataset)\n",
        "                \n",
        "    def __len__(self):\n",
        "        return len(self.images_list) # Return the size of dataset\n",
        "\n",
        "    def __getitem__(self, idx): # Get an index\n",
        "        nii_image = self.images_list[idx] # Read image \n",
        "        data = torch.from_numpy(nii_image) # Convert image to tensor \n",
        "        nii_label = self.label_list[idx] # Read label \n",
        "        target = torch.from_numpy(nii_label) # Convert label to tensor\n",
        "        return data, target"
      ],
      "metadata": {
        "id": "oXnJ2J7n-hJ_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L_path_1 = glob.glob('/content/drive/MyDrive/MedicalImage-Team/LiTS17/LITS Challenge/Training Batch 1/segmentation*.nii')\n",
        "L_path_2 = glob.glob('/content/drive/MyDrive/MedicalImage-Team/LiTS17/LITS Challenge/Training Batch 2/segmentation*.nii')\n",
        "L_path = L_path_1 + L_path_2\n",
        "len(L_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ56LmQTBpsE",
        "outputId": "42f0eac2-9452-4731-84c8-f03ce57a9928"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I_path_1 = glob.glob('/content/drive/MyDrive/MedicalImage-Team/LiTS17/LITS Challenge/Training Batch 1/volume*.nii')\n",
        "I_path_2 = glob.glob('/content/drive/MyDrive/MedicalImage-Team/LiTS17/LITS Challenge/Training Batch 2/volume*.nii')\n",
        "I_path = I_path_1 + I_path_2\n",
        "len(I_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjlZ8dQ1bS6u",
        "outputId": "d8b18c7a-fe5b-4768-8bc4-274b5f579d8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 30\n",
        "MyDataset = MyCustomDataset(I_path,L_path,k)\n",
        "len(MyDataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A20Fs4UIBQ0V",
        "outputId": "916b7aad-e4f2-4908-8137-b75ea81e4cc7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_properties"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXtC4GCyIL6r",
        "outputId": "5b9ce223-5836-4f21-db83-d3835a616d36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'dana_num': 0, 'first_layer': 45, 'last_layer': 73},\n",
              " {'dana_num': 1, 'first_layer': 45, 'last_layer': 73},\n",
              " {'dana_num': 2, 'first_layer': 334, 'last_layer': 472},\n",
              " {'dana_num': 3, 'first_layer': 303, 'last_layer': 471},\n",
              " {'dana_num': 4, 'first_layer': 347, 'last_layer': 596},\n",
              " {'dana_num': 5, 'first_layer': 312, 'last_layer': 487},\n",
              " {'dana_num': 6, 'first_layer': 305, 'last_layer': 490},\n",
              " {'dana_num': 7, 'first_layer': 324, 'last_layer': 500},\n",
              " {'dana_num': 8, 'first_layer': 332, 'last_layer': 510},\n",
              " {'dana_num': 9, 'first_layer': 343, 'last_layer': 515},\n",
              " {'dana_num': 10, 'first_layer': 295, 'last_layer': 475},\n",
              " {'dana_num': 11, 'first_layer': 272, 'last_layer': 438},\n",
              " {'dana_num': 12, 'first_layer': 265, 'last_layer': 453},\n",
              " {'dana_num': 13, 'first_layer': 272, 'last_layer': 413},\n",
              " {'dana_num': 14, 'first_layer': 260, 'last_layer': 398},\n",
              " {'dana_num': 15, 'first_layer': 240, 'last_layer': 372},\n",
              " {'dana_num': 16, 'first_layer': 267, 'last_layer': 453},\n",
              " {'dana_num': 17, 'first_layer': 328, 'last_layer': 525},\n",
              " {'dana_num': 18, 'first_layer': 354, 'last_layer': 542},\n",
              " {'dana_num': 19, 'first_layer': 324, 'last_layer': 511},\n",
              " {'dana_num': 20, 'first_layer': 354, 'last_layer': 547},\n",
              " {'dana_num': 21, 'first_layer': 260, 'last_layer': 434},\n",
              " {'dana_num': 22, 'first_layer': 103, 'last_layer': 148},\n",
              " {'dana_num': 23, 'first_layer': 262, 'last_layer': 377},\n",
              " {'dana_num': 24, 'first_layer': 141, 'last_layer': 258},\n",
              " {'dana_num': 25, 'first_layer': 341, 'last_layer': 572},\n",
              " {'dana_num': 26, 'first_layer': 255, 'last_layer': 446},\n",
              " {'dana_num': 27, 'first_layer': 331, 'last_layer': 559},\n",
              " {'dana_num': 28, 'first_layer': 27, 'last_layer': 124},\n",
              " {'dana_num': 29, 'first_layer': 44, 'last_layer': 157},\n",
              " {'dana_num': 30, 'first_layer': 60, 'last_layer': 182},\n",
              " {'dana_num': 31, 'first_layer': 19, 'last_layer': 85},\n",
              " {'dana_num': 32, 'first_layer': 19, 'last_layer': 137},\n",
              " {'dana_num': 33, 'first_layer': 37, 'last_layer': 132},\n",
              " {'dana_num': 34, 'first_layer': 48, 'last_layer': 145},\n",
              " {'dana_num': 35, 'first_layer': 6, 'last_layer': 121},\n",
              " {'dana_num': 36, 'first_layer': 11, 'last_layer': 99},\n",
              " {'dana_num': 37, 'first_layer': 22, 'last_layer': 120},\n",
              " {'dana_num': 38, 'first_layer': 33, 'last_layer': 128},\n",
              " {'dana_num': 39, 'first_layer': 9, 'last_layer': 247},\n",
              " {'dana_num': 40, 'first_layer': 26, 'last_layer': 115},\n",
              " {'dana_num': 41, 'first_layer': 4, 'last_layer': 107},\n",
              " {'dana_num': 42, 'first_layer': 6, 'last_layer': 120},\n",
              " {'dana_num': 43, 'first_layer': 39, 'last_layer': 151},\n",
              " {'dana_num': 44, 'first_layer': 1, 'last_layer': 113},\n",
              " {'dana_num': 45, 'first_layer': 12, 'last_layer': 70},\n",
              " {'dana_num': 46, 'first_layer': 28, 'last_layer': 68},\n",
              " {'dana_num': 47, 'first_layer': 126, 'last_layer': 210},\n",
              " {'dana_num': 48, 'first_layer': 108, 'last_layer': 176},\n",
              " {'dana_num': 49, 'first_layer': 102, 'last_layer': 174},\n",
              " {'dana_num': 50, 'first_layer': 96, 'last_layer': 159},\n",
              " {'dana_num': 51, 'first_layer': 96, 'last_layer': 151},\n",
              " {'dana_num': 52, 'first_layer': 100, 'last_layer': 155},\n",
              " {'dana_num': 53, 'first_layer': 55, 'last_layer': 98},\n",
              " {'dana_num': 54, 'first_layer': 61, 'last_layer': 89},\n",
              " {'dana_num': 55, 'first_layer': 94, 'last_layer': 185},\n",
              " {'dana_num': 56, 'first_layer': 135, 'last_layer': 225},\n",
              " {'dana_num': 57, 'first_layer': 204, 'last_layer': 352},\n",
              " {'dana_num': 58, 'first_layer': 114, 'last_layer': 196},\n",
              " {'dana_num': 59, 'first_layer': 132, 'last_layer': 208},\n",
              " {'dana_num': 60, 'first_layer': 157, 'last_layer': 235},\n",
              " {'dana_num': 61, 'first_layer': 125, 'last_layer': 185},\n",
              " {'dana_num': 62, 'first_layer': 89, 'last_layer': 177},\n",
              " {'dana_num': 63, 'first_layer': 60, 'last_layer': 96},\n",
              " {'dana_num': 64, 'first_layer': 114, 'last_layer': 217},\n",
              " {'dana_num': 65, 'first_layer': 281, 'last_layer': 481},\n",
              " {'dana_num': 66, 'first_layer': 48, 'last_layer': 83},\n",
              " {'dana_num': 67, 'first_layer': 82, 'last_layer': 160},\n",
              " {'dana_num': 68, 'first_layer': 91, 'last_layer': 154},\n",
              " {'dana_num': 69, 'first_layer': 134, 'last_layer': 229},\n",
              " {'dana_num': 70, 'first_layer': 209, 'last_layer': 318},\n",
              " {'dana_num': 71, 'first_layer': 55, 'last_layer': 90},\n",
              " {'dana_num': 72, 'first_layer': 28, 'last_layer': 85},\n",
              " {'dana_num': 73, 'first_layer': 61, 'last_layer': 110},\n",
              " {'dana_num': 74, 'first_layer': 68, 'last_layer': 99},\n",
              " {'dana_num': 75, 'first_layer': 46, 'last_layer': 82},\n",
              " {'dana_num': 76, 'first_layer': 83, 'last_layer': 157},\n",
              " {'dana_num': 77, 'first_layer': 57, 'last_layer': 92},\n",
              " {'dana_num': 78, 'first_layer': 122, 'last_layer': 180},\n",
              " {'dana_num': 79, 'first_layer': 77, 'last_layer': 135},\n",
              " {'dana_num': 80, 'first_layer': 127, 'last_layer': 202},\n",
              " {'dana_num': 81, 'first_layer': 214, 'last_layer': 324},\n",
              " {'dana_num': 82, 'first_layer': 323, 'last_layer': 492},\n",
              " {'dana_num': 83, 'first_layer': 386, 'last_layer': 600},\n",
              " {'dana_num': 84, 'first_layer': 393, 'last_layer': 650},\n",
              " {'dana_num': 85, 'first_layer': 335, 'last_layer': 610},\n",
              " {'dana_num': 86, 'first_layer': 343, 'last_layer': 622},\n",
              " {'dana_num': 87, 'first_layer': 422, 'last_layer': 636},\n",
              " {'dana_num': 88, 'first_layer': 361, 'last_layer': 535},\n",
              " {'dana_num': 89, 'first_layer': 367, 'last_layer': 559},\n",
              " {'dana_num': 90, 'first_layer': 301, 'last_layer': 483},\n",
              " {'dana_num': 91, 'first_layer': 332, 'last_layer': 520},\n",
              " {'dana_num': 92, 'first_layer': 355, 'last_layer': 533},\n",
              " {'dana_num': 93, 'first_layer': 265, 'last_layer': 498},\n",
              " {'dana_num': 94, 'first_layer': 395, 'last_layer': 646},\n",
              " {'dana_num': 95, 'first_layer': 294, 'last_layer': 556},\n",
              " {'dana_num': 96, 'first_layer': 345, 'last_layer': 643},\n",
              " {'dana_num': 97, 'first_layer': 407, 'last_layer': 638},\n",
              " {'dana_num': 98, 'first_layer': 388, 'last_layer': 628},\n",
              " {'dana_num': 99, 'first_layer': 379, 'last_layer': 604},\n",
              " {'dana_num': 100, 'first_layer': 387, 'last_layer': 662},\n",
              " {'dana_num': 101, 'first_layer': 387, 'last_layer': 645},\n",
              " {'dana_num': 102, 'first_layer': 322, 'last_layer': 634},\n",
              " {'dana_num': 103, 'first_layer': 422, 'last_layer': 635},\n",
              " {'dana_num': 104, 'first_layer': 273, 'last_layer': 466},\n",
              " {'dana_num': 105, 'first_layer': 387, 'last_layer': 625},\n",
              " {'dana_num': 106, 'first_layer': 326, 'last_layer': 493},\n",
              " {'dana_num': 107, 'first_layer': 266, 'last_layer': 513},\n",
              " {'dana_num': 108, 'first_layer': 412, 'last_layer': 616},\n",
              " {'dana_num': 109, 'first_layer': 288, 'last_layer': 481},\n",
              " {'dana_num': 110, 'first_layer': 344, 'last_layer': 532},\n",
              " {'dana_num': 111, 'first_layer': 231, 'last_layer': 463},\n",
              " {'dana_num': 112, 'first_layer': 291, 'last_layer': 481},\n",
              " {'dana_num': 113, 'first_layer': 409, 'last_layer': 578},\n",
              " {'dana_num': 114, 'first_layer': 364, 'last_layer': 578},\n",
              " {'dana_num': 115, 'first_layer': 411, 'last_layer': 596},\n",
              " {'dana_num': 116, 'first_layer': 447, 'last_layer': 665},\n",
              " {'dana_num': 117, 'first_layer': 278, 'last_layer': 537},\n",
              " {'dana_num': 118, 'first_layer': 196, 'last_layer': 317},\n",
              " {'dana_num': 119, 'first_layer': 201, 'last_layer': 332},\n",
              " {'dana_num': 120, 'first_layer': 153, 'last_layer': 272},\n",
              " {'dana_num': 121, 'first_layer': 204, 'last_layer': 316},\n",
              " {'dana_num': 122, 'first_layer': 156, 'last_layer': 273},\n",
              " {'dana_num': 123, 'first_layer': 185, 'last_layer': 299},\n",
              " {'dana_num': 124, 'first_layer': 165, 'last_layer': 276},\n",
              " {'dana_num': 125, 'first_layer': 163, 'last_layer': 274},\n",
              " {'dana_num': 126, 'first_layer': 176, 'last_layer': 288},\n",
              " {'dana_num': 127, 'first_layer': 477, 'last_layer': 703},\n",
              " {'dana_num': 128, 'first_layer': 300, 'last_layer': 591},\n",
              " {'dana_num': 129, 'first_layer': 37, 'last_layer': 313},\n",
              " {'dana_num': 130, 'first_layer': 336, 'last_layer': 576}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "data = np.array(nib.load(I_path[i]).dataobj)\n",
        "print(I_path[i])\n",
        "np.shape(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_O89fL5DkD5",
        "outputId": "a8862a0c-ba21-472e-b4d9-a08d9f4e9d00"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MedicalImage-Team/LiTS17/LITS Challenge/Training Batch 1/volume-1.nii\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 512, 123)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(I_path[i])\n",
        "print(np.shape(data))"
      ],
      "metadata": {
        "id": "JsijdEm-GWCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "My_Dataloader = DataLoader(MyDataset, batch_size=4, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "T2I3jJRTEnAc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "AtuGlJfEEq80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = UNet3D(in_channels=1, out_channels=1)"
      ],
      "metadata": {
        "id": "SYRwGjyU8dL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i1D0RK0iEt81"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}